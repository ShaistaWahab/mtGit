# A Human Study Of Comprehension And Code Summerization.



## Authors:

Sean Stepleton, Yashmeet Gambhir, Alexander LeClair, Zarchary Eberhart, Westley Weimer, Kevin Leach and Yu Huang.

>Conerence:  
Tue 14 Jul 2020 02:00 - 02:15 at ICPC - Session 4: Summalization Chair(s): Venera Arnaoudova

[Paper link](https://dijkstra.eecs.umich.edu/kleach/icpc2020-code-summarization.pdf "click here to view paper")

>Media link:  https://youtu.be/V9h5mPHf4Hg


# **Introduction And Motivation**:

Sean Stepleton, Yashmeet Gambhir, Alexander LeClair, Zarchary Eberhart, Westley Weimer, Kevin Leach and Yu Huang from different universities grouped together and illuminated about the human summarization from different techniques. In the past research they investigated the techniques for automatically documented code to improve comprehensibility but it seems difficult to summarize machine techniques, however their summarization techniques are hard to understand by human comprehension and productivity, and assessed these by using BLEU and ROUGE but still they didnâ€™t got accurate result. So meanwhile they just arranged a session in which they conducted tqo participants groups for answering the comprehension questions from human study and machine generated by Java method. However they found human written much better (P=02) than machine summarization but still there is no evidence that human written text is perfect. Human written and Machine summarization both has different qualities. 
Genuinely source code plays a vital role in facilitating program comprehension. Short descriptive comments improve ability to answer the source code questions. Source code helps developers to answer the comprehension questions about source code to maintain the various programming tasks. Developers created task by source code for the first time, while investigating and existing the project, Developers maintained a large codebases. Source code comments are still imperfect and incorrect in practice. In the study of professional programmers, xia et a has many difficulties in program comprehension. In order to solve these problems, developers generated techniques to automatically generated summary comments for source code  by using Heuristics and Templates. Recently a new code summarization technique is emerged that is deep learning and publicly available code which is capable of code summaries by human comment but still there is need to some improvements as they generally did not express what is the exact function of this and for which purpose it is intented. It shows no co-relation between metrics and automatic code summarization. This is done by the programmers for human study because they found human written text is much better than machine text. The purpose of this was to illustrate the statical evaluation metrics such as BLEU and ROUGE. which compare generated text to the preference without considering its meaning.

# **Result**:
    
    In this whole research we found that written summaries by developers are better than machine text. Developers are in favour of summaries quality inspite of human written text or machine text.

